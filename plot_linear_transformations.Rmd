---
title: "Untitled"
author: "Kevin Havis"
date: "2025-01-29"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Geometric transformation of shapes using matrix multiplication

> Create a simple shape (like a square or triangle) using point plots in R. Implement R code to apply different transformations (scaling, rotation, reflection) to the shape by left multiplying a transformation matrix by each of the point vectors. Demonstrate these transformations through animated plots.

```{r}
source('linear_transformation_viz.R')
library(matlib)
library(gganimate)
```

```{r}

# Rotate 90 degrees counter clockwise
rotate_90 <- matrix(c(1,0,
                      0,-1), nrow=2, byrow=TRUE)

# Rotate 180 degrees counter clockwise
rotate_180 <- matrix(c(-1,0,
                        0,1), nrow=2, byrow=TRUE)

# Rotate 90 degrees clockwise
rotate_270 <- matrix(c(0,1,
                       1,0), nrow=2, byrow=TRUE)

# Apply a right-ward shear
shear <- matrix(c(1,0,
                  1,1), nrow=2, byrow=TRUE)

# Reflect across line of symmetry
mirror <- matrix(c(-1,0,
                    0,-1), nrow=2, byrow=TRUE)

# Reflect across y axis
reflect_y <- matrix(c(-1, 0,
                      0, 1), nrow=2, byrow=TRUE)

# Reflect across x axis
reflect_x <- matrix(c(1,0,
                      0,-1), nrow=2, byrow=TRUE)
                

```

```{r}
vector <- c(2, 2)
animate(rotate_90, vector)
```

```{r}
# Create a cartesian coordinate system
p <- plot_coord_system()


interpolate_matrix_transformation <- function(vectors, frames, transformation_matrix){
  
  # Initialize an identity matrix to later interpolate
  # This way we can interpolate any transformation matrix we want
  i_matrix <- matrix(c(1, 0,
                0, 1), nrow=2, byrow=TRUE)

  # Init df to store transformed vectors
  transformed_vectors <- data.frame()
  
  # For each animation frame
  for (f in 1:frames){

    scale <- f / frames
    
    # progressively interpolate the identify matrix
    # and multiply by the transformation matrix
    interpolation_matrix <- (1 - scale) * i_matrix +
                          scale * transformation_matrix
    
    
    # Use the now interpolated transformation matrix to calculate new points
    transformed_shape <- vectors %*% interpolation_matrix
    
    # Get the new vectors and the frame they were created on
    step_vectors <- as.data.frame(transformed_shape)
    step_vectors$frame <- f
    
    # Append dataframe
    transformed_vectors <- rbind(transformed_vectors, step_vectors)
    
  }
  

  colnames(transformed_vectors) <- c('x', 'y', 'frame')
  
  return(transformed_vectors)
  
}
  

# Create a "square"
square <- data.frame(x=c(-1, 1, 1, -1),
                     y=c(-1, -1, 1, 1))
# Convert to matrix
m <- as.matrix(square)

# Set animation params
frames <- 60
scale <- seq(1, -1, length.out = frames)

# Build dataframe of interpolated vectors
transformed_vectors <- interpolate_matrix_transformation(m, frames, reflect_y)

```

```{r}
# Build coordinate plot
# and run the animation

p_anim <- ggplot(transformed_vectors, aes(x=x, y=y)) +
    geom_point(size = 4, aes(color=frame), shape = "square") +
    geom_hline(yintercept = -10:10, color = "lightgray", linetype = "dotted") +  # Grid lines
    geom_vline(xintercept = -10:10, color = "lightgray", linetype = "dotted") +  
    geom_hline(yintercept = 0, color = "black", linetype = "solid", size = 1) +  # X-axis
    geom_vline(xintercept = 0, color = "black", linetype = "solid", size = 1) +  # Y-axis
    scale_x_continuous(breaks = -10:10) + 
    scale_y_continuous(breaks = -10:10) +
    coord_cartesian(xlim = c(-10, 10), ylim = c(-10, 10)) +
    theme_minimal() +
    theme(panel.grid = element_blank()) +
    transition_states(frame, transition_length = 1, state_length = 1)

print(p_anim)

```

## Matrix properties and decomposition

### Prove that $AB \neq BA$

Matrices do not enjoy the commutative multiplicative property, unlike arithmetic algebra.

We can easily prove that $AB \neq BA$ empirically with a few lines of R code.

```{r}
A <- matrix(c(1,2,3,
              4,5,6,
              7,8,9), nrow=3, byrow=TRUE)

B <- matrix(c(1, 1, 1,
              2, 2, 2,
              3, 3, 3), nrow=3, byrow=TRUE)

AB <- A%*%B

BA <- B%*%B

print(all.equal(AB, BA))

```

### Prove that $A\^TA$ is always symmetric

A transpose of a matrix of rows $m$ and columns $n$ is simply re-ordering a matrix such that $A^T$ is composed of $n$ rows and $m$ columns. As such, the matrix maintains symmetry *with respect to the main diagonal*.

Again, we can show this using a concrete example, even with a non-square matrix.

```{r}

S <- matrix(c('a', 'b', 'c', 'x',
              'd', 'e', 'f', 'y',
              'g', 'h', 'i', 'z'), nrow=3, byrow=TRUE)

# transpose
print(t(S))

```

As we can see, the main diagonal of $a,e,i$ is retained, demonstrating symmetry throughout the transpose.

### Prove that the determinant of $A^TA$ is non-negative

The determinant is the scaling factor by which vector space is expanded after a matrix transformation. A negative determinant would indicate a reflection, meaning vector space is "stretched" towards either or both the negative x and y axis (given a standard two dimensional coordinate system).

Considering a two dimensional square matrix, $A=\begin{bmatrix}a&b\\c&d\end{bmatrix}$, we can calculate the determinant as $det|A| = A_{11}A_{22} - A_{12}A_{21} = ad - bc$.

Considering the transpose $A^T=\begin{bmatrix}a&c\\b&d\end{bmatrix}$, we can see that the determinant is non-negative given the commutative property of scalar multiplication; $det|A^T| = A^{T}_{11}A^{T}_{22} - A^{T}_{21}A^{T}_{12} = ad - cb$.

This is demonstrated in code below.

```{r}
A <- matrix(c(1, 2,
              3, 4), nrow=2, byrow=TRUE)

A_t <- t(A)

print(det(A_t %*% A))
```

### Image Singular Value Decomposition

We can use singular value decomposition, or SVD, to compress images. This works by analyzing the image as a matrix of pixels, each with a value of saturation. In the case of a color image, you would also have red, green, and blue matrices, but we will work with a simpler greyscale image in this example.

```{r}
library(magick)

# Read a grayscale image
image <- image_read('image.png')
grey_image <- image_convert(image, colorspace = 'gray')

image_matrix <- as.numeric(image_data(grey_image))

dim(image_matrix) <- c(image_info(grey_image)$width, image_info(grey_image)$height)

# Factorize the image matrix A using svd()

svd_matrix <- svd(image_matrix)

# Compress the image using top k singular values and vectors

k <- 10

compress_matrix <- svd_matrix$u[, 1:k] %*% diag(svd_matrix$d[1:k]) %*% t(svd_matrix$v[, 1:k])

# Visualize the result

compress_matrix_3d <- array(compress_matrix, dim = c(dim(compress_matrix)[1], dim(compress_matrix)[2], 3))

compress_image <- image_read(compress_matrix_3d)

print(image_append(c(grey_image, compress_image)))

```

## Matrix rank, properties, and eigenspace

### Determine the rank of the given matrix

> Find the rank of matrix A

*Note: we assume A is a coefficient matrix, not an augmented matrix in this example.*

In linear algebra, **rank** refers to the number of dimensions affected by a matrix transformation. If a matrix transformation retains all dimensions of a vector space, it is said to be **full rank**. A lesser rank occurs when a matrix transformation forces vector space into a smaller dimension; for example, a cube to a plane, a plane to a line, or a line to a point.

We calculate rank by first performing Gaussian elimination to get the matrix into reduced row echelon form. From there, the number of pivot columns is equivalent to the rank, as each pivot column represents linear independence for that given dimension.

```{r}
# Determine the rank of A
A <- matrix(c(2, 4, 1, 3,
              -2, -3, 4, 1,
              5, 6, 2, 8,
              -1, -2, 3, 7), nrow=4, byrow=TRUE)

A_rref <- echelon(A)

print(A_rref)

```

In this case, we can say that the rank of $A$ is four.

> Explain what the rank tells us about the linear independence of the rows and columns of A

A is full rank, as mentioned above, we have appropriate pivot values in each of our dimensions, which tells us the system is linearly independent and therefore full rank.

> Identify any linear dependencies among the rows or columns

If we saw any rows or columns that were strictly all zeroes, we could consider them to be free variables, and as such, linearly dependent.There are none in this case.

### Matrix rank boundaries

> Given an $mxn$ matrix where $m>n$, determine the maximum and minimum possible rank, assuming that the matrix is non-zero.

A matrix with more rows than columns indicate that we

We cannot have a higher rank than we have dimensions, so the maximum rank is $r_{max}\le n$

If the matrix is non-zero, the smallest it could possibly be is $1x1$. Therefore, rank must also be $r_{min}=1$.

> Prove that the rank of a matrix equals the dimension of its row space (or column space). Provide an example to illustrate the concept.

Given a linear system of equations $LS$ represented by matrix $A$ such that $LS(A)$;

1.  A homogeneous system $LS(A, 0)$ is always consistent
2.  The trivial ($0$) solution to $LS(A,0)$ is unique
3.  The reduced row echelon form $B$ of an augmented matrix $A$ is row equivalent
4.  $B$ has pivot columns $r$ which must be less than or equal to the number of columns $n$
5.  $r$ is equal to $n$ when the system has a unique solution
6.  $LS(A, 0)$ is consistent and has a unique solution, so for matrix $A$, $r$ must be equal to $n$
7.  Rank of A $r(A)$ is equal to $r$, so $r(A) = r_A = A_n$
8.  The transpose $A^T$ of matrix $A$ has rows $m$ and columns $n$ such that $A^{T}_{m} = A_n$
9.  The rank $r$ of a matrix is equal to the rank of the transpose, so $r_A = r_{A^T}$
10. $r_A = A_m$, or $r_A = A_n$

Example given $4x_1 + 2x_2  = 0$ and $3x_1 + x2 = 0$

```{r}
# Initialize matrix A
A <- matrix(c(4, 2,
              3, 1), nrow=2, byrow=TRUE)

# Compute RREF form B
B <- echelon(A)

# The number of pivot columns is X
print(B)

# Rank is equal to the number of rows
r <- NROW(A)

# Transpose of A
A_t <- t(A)

# Assert r is equal to rows
print(identical(r,nrow(A_t)))
```

### Rank and row reduction

> Determine the rank of matrix $B$. Perform a row reduction on matrix $B$ and describe how it helps in finding the rank. Discuss any special properties of matrix $B$ (e.g., is it a rank-deficient matrix?).

```{r}
# Initialize matrix
B <- matrix(c(2, 5, 7,
              4, 10, 14,
              1, 2.5, 2.5), nrow=3, byrow=TRUE)

# 
B_rref <- echelon(B)

print(B_rref)
```

### Compute the eigenvalues and eigenvectors

> Find the eigenvalues and eigenvectors of the matrix $A$. Write out the characteristic polynomial and show your solution step by step. After finding the eigenvalues and eigenvectors, verify that the eigenvectors are linearly independent. If they are not, explain why.

$$
A =\begin{bmatrix}
3&1&2\\
0&5&4\\
0&0&2
\end{bmatrix}
$$ 

First we perform row operations to get the row reduced echelon form of $A$

$$ 
\begin{align}

\begin{bmatrix}
3&1&2\\
0&5&4\\
0&0&2
\end{bmatrix} \to -4R_1 + R_2

\begin{bmatrix}3&1&2\\0&1&4\\0&0&2\end{bmatrix} \\

-R_2 + R_1 \to

\begin{bmatrix}3&0&-2\\0&1&4\\0&0&2\end{bmatrix} \\

R_1 + R_3 \to

\begin{bmatrix}3&0&0\\0&1&4\\0&0&2\end{bmatrix} \\

\frac{1}{3}R_1 \to

\begin{bmatrix}1&0&0\\0&1&4\\0&0&2\end{bmatrix} \\

-2R_3 + R_2 \to

\begin{bmatrix}1&0&0\\0&1&0\\0&0&2\end{bmatrix} \\

\frac{1}{2}R_3 \to
\begin{bmatrix}1&0&0\\0&1&0\\0&0&1\end{bmatrix}
\end{align}
$$
In this case matrix $A$ can reduce to the identity matrix. We now determine the characteristic polynomial by calculating $p_A(x)=det(A-xI_3)$

$$
p_A(\lambda)=det(A-\lambda I_3) = det(\begin{bmatrix}1-\lambda&0&0\\0&1-\lambda&0\\0&0&1-\lambda\end{bmatrix})
$$
We have a diagonal matrix, so we can calculate the determinant as the product of its diagonal; $p_A(\lambda)=(1-x)(1-x)(1-x)$.

This is also our characteristic polynomial, and as expected, the eigenvalues are 1, which we would expect of any identity matrix. Since the eigenvalue of $1$ appears three times in our characteristic polynomial, we can also state the algebraic multiplicity is three.

Our eigenvectors can then be defined as $A\overrightarrow{x} = \lambda \overrightarrow{x}$. Since our matrix $A$ is an identity matrix, we can say $\overrightarrow{x} = \lambda \overrightarrow{x}$. Finally, since we have found $\lambda = 1$, we can say $\overrightarrow{x}=\overrightarrow{x}$, which is the same as saying that any vector of A is an eigenvector.

Since any vector of A is an eigenvector, we can say that the eigenvectors are linearly independent.This means they span all of their dimensional space

```{r}
A <- matrix(c(3, 1, 2,
              0, 5, 4,
              0, 0, 2), nrow=3, byrow=TRUE)

```

### Diagonalization of matrix

> Determine if matrix $A$ can be diagonalized. If it can, find the diagonal matrix and the matrix of eigenvectors that diagonalizes $A$.

> Discuss the geometric interpretation of the eigenvectors and eigenvalues in the context of transformations. For instance, how does matrix stretch, shrink, or rotate vectors in $\mathbb{R}^3$?

## Project: Eigenfaces from the "Labeled Faces in the Wild" dataset

Preprocess the Images:

    Convert the images to grayscale and resize them to a smaller size (e.g., 64x64) to reduce computational complexity.
    Flatten each image into a vector.

Apply PCA:

    Compute the PCA on the flattened images.
    Determine the number of principal components required to account for 80% of the variability.

Visualize Eigenfaces:

    Visualize the first few eigenfaces (principal components) and discuss their significance.
    Reconstruct some images using the computed eigenfaces and compare them with the original images.


I sampled four images from the dataset due to drive space, but the below code can be scaled to fit any number of images.

We are essentially using `magick` to help with the image processing. We read each image in the directory, preprocess it into greyscale and downscale to 64x64, then get the numerica representation as a vector, storing each in a list. Then it is simple to use `prcomp` to calculate the PCA summary for us for each image.

During my first attempts, I realized that some images had zero pixels. This meant we needed to perform some preprocessing to remove null elements so we could perform the PCA reliably. This is necessary, as during the covariance matrix computation, you'll get zero variance which causes issues with the standaridzation for the rest of the PCA computation. I used Claude AI to help refactor my code so that we could dynamically and elegantly handle images that had this issue.It suggested using `var_cols <- apply(mat, 2, var) > 0` to identify columns with non-zero variance, which I've implemented below.

```{r}
library(magick)

# Read all iamges
image_vector_list <- list.files(path = "lfw/", 
                               pattern = "\\.(jpg)$",
                               full.names = TRUE) |>
  lapply(function(file) {
    # Read each image
    image_read(file) |>
    # Convert to greyscale
    image_convert(colorspace = 'gray') |>
    # Rescale image
    image_scale('64') |>
    # Get numeric image data
    image_data() |>
    as.numeric()
  })
```


```{r}
# Perform PCA on each vector
pca_list <- lapply(image_vector_list, function(image_vec) {
    # Reshape vector to matrix
    mat <- matrix(image_vec, nrow = 64, ncol = 64)
    
    # Check for columns with non-zero variance, otherwise we get error
    var_cols <- apply(mat, 2, var) > 0
    
    # If there are any columns with variance
    if(any(var_cols)) {
        # Only perform PCA on columns with variance
        mat_filtered <- mat[, var_cols, drop = FALSE]
        pca_result <- prcomp(mat_filtered, center = TRUE, scale. = TRUE)
        return(list(
            pca = pca_result,
            used_cols = which(var_cols)  # Keep track of which columns we used
        ))
    } else {
        # Return NULL or some indicator if no variance in image
        return(NULL)
    }
})

# Remove any NULL results (images where PCA couldn't be performed)
pca_list <- pca_list[!sapply(pca_list, is.null)]
```

With the PCA computed for each image, we can view the results and see for each how many princple components are need to explain more than 80% of the variability, by reviewing the `Cumulative Proportion` of each.

```{r}
# The first image needs PC5
print(summary(pca_list[[1]]$pca))
```
```{r}
# The second image needs PC 5 as well
print(summary(pca_list[[2]]$pca))
```

```{r}
# The third image only needs PC4
print(summary(pca_list[[3]]$pca))
```

```{r}
# The fourth image also only needs PC4
print(summary(pca_list[[4]]$pca))
```

Now to visualize how much information we get at each principal component. 

```{r}
# Function to reconstruct image from n principal components
reconstruct_image <- function(pca_result, n_components) {
    # Get the eigenvectors for first n components
    eigenvec <- pca_result$pca$rotation[, 1:n_components, drop = FALSE]
    
    # Get the scores for first n components
    scores <- pca_result$pca$x[, 1:n_components, drop = FALSE]
    
    # Reconstruct using matrix multiplication
    reconstructed <- scores %*% t(eigenvec)
    
    # Un-scale and un-center (reverse the standardization)
    # Check if scale is TRUE/FALSE rather than a vector
    if(!is.null(pca_result$pca$scale.) && pca_result$pca$scale.) {
        reconstructed <- scale(reconstructed, 
                             center = FALSE, 
                             scale = 1/pca_result$pca$sdev[1:n_components])
    }
    
    # Add back the center
    if(!is.null(pca_result$pca$center)) {
        reconstructed <- t(t(reconstructed) + pca_result$pca$center)
    }
    
    # Create full-size image matrix
    full_image <- matrix(0, nrow = 64, ncol = 64)
    full_image[, pca_result$used_cols] <- reconstructed
    
    return(full_image)
}

# Get first image's PCA result
first_pca <- pca_list[[1]]

# Original image for comparison
original_mat <- matrix(image_vector_list[[1]], nrow = 64, ncol = 64)

# Set up plotting parameters
par(mfrow = c(2,3))

# Original
image(original_mat, main = "Original")

# Reconstructions with different numbers of PCs
for(n_pc in c(1, 5, 10, 20, 50)) {
    reconstructed <- reconstruct_image(first_pca, n_pc)
    image(reconstructed, 
          main = paste("Using", n_pc, "PCs"))
}

# Calculate reconstruction error
for(n_pc in c(1, 5, 10, 20, 50)) {
    reconstructed <- reconstruct_image(first_pca, n_pc)
    mse <- mean((original_mat - reconstructed)^2)
    print(paste(n_pc, "PCs:", round(mse, 6)))
}
```

