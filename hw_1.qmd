---
title: "DATA605_hw_1"
author: "Kevin Havis"
date: "2025-02-16"
format: pdf

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(matlib)
library(gganimate)
library(pracma)
```

## Geometric transformation of shapes using matrix multiplication

> Create a simple shape (like a square or triangle) using point plots in R. Implement R code to apply different transformations (scaling, rotation, reflection) to the shape by left multiplying a transformation matrix by each of the point vectors. Demonstrate these transformations through animated plots.

We'll first start by identifying some common two dimensional transformations, such as rotation and shears. By representing these transformations as matrices, we'll be able to use any of these in our upcoming animation function.

```{r}
# Rotate 90 degrees counter clockwise
rotate_90 <- matrix(c(1,0,
                      0,-1), nrow=2, byrow=TRUE)

# Rotate 180 degrees counter clockwise
rotate_180 <- matrix(c(-1,0,
                        0,1), nrow=2, byrow=TRUE)

# Rotate 90 degrees clockwise
rotate_270 <- matrix(c(0,1,
                       1,0), nrow=2, byrow=TRUE)

# Apply a right-ward shear
shear <- matrix(c(1,0,
                  1,1), nrow=2, byrow=TRUE)

# Reflect across line of symmetry
mirror <- matrix(c(-1,0,
                    0,-1), nrow=2, byrow=TRUE)

# Reflect across y axis
reflect_y <- matrix(c(-1, 0,
                      0, 1), nrow=2, byrow=TRUE)

# Reflect across x axis
reflect_x <- matrix(c(1,0,
                      0,-1), nrow=2, byrow=TRUE)

```

We'll now build the transformation and animations. To properly animate, we need to interpolate through the transformation matrix, performing partial transformations until the entire transformation is complete.

We will do this by establishing a frame rate, looping through the transformation, and incrementing the transformation by the current frame for each loop. Mathematically, we are using the value of the current frame as a scalar by which we scale the transformation matrix, incrementing each loop by adding it back to the transformation matrix.

We calculate and store these intermediate values in a dataframe to make it easy to animate later.

```{r}
interpolate_matrix_transformation <- function(vectors, frames, transformation_matrix){
  
  # Initialize an identity matrix to later interpolate
  # This way we can interpolate any transformation matrix we want
  i_matrix <- matrix(c(1, 0,
                0, 1), nrow=2, byrow=TRUE)

  # Init df to store transformed vectors
  transformed_vectors <- data.frame()
  
  # For each animation frame
  for (f in 1:frames){

    scale <- f / frames
    
    # progressively interpolate the identify matrix
    # and multiply by the transformation matrix
    interpolation_matrix <- (1 - scale) * i_matrix +
                          scale * transformation_matrix
    
    
    # Use the now interpolated transformation matrix to calculate new points
    transformed_shape <- vectors %*% interpolation_matrix
    
    # Get the new vectors and the frame they were created on
    step_vectors <- as.data.frame(transformed_shape)
    step_vectors$frame <- f
    
    # Append dataframe
    transformed_vectors <- rbind(transformed_vectors, step_vectors)
    
  }
  # names the dataframe columns
  colnames(transformed_vectors) <- c('x', 'y', 'frame')
  return(transformed_vectors)
}
```

Now with our function defined, we'll define a "square" as a matrix of vector points and build the dataframe containing all the incremental vector values we'll need in order to animate.

```{r}
# Create a "square"
square <- data.frame(x=c(-1, 1, 1, -1),
                     y=c(-1, -1, 1, 1))
# Convert to matrix
m <- as.matrix(square)

# Set animation params
frames <- 60
scale <- seq(1, -1, length.out = frames)

# Build dataframe of interpolated vectors
transformed_vectors <- interpolate_matrix_transformation(m, frames, reflect_y)

```

Finally, we'll plot a simple x-y coordinate system to display the transformation upon.

```{r}
# Build coordinate plot
# and run the animation

p_anim <- ggplot(transformed_vectors, aes(x=x, y=y)) +
    geom_point(size = 4, aes(color=frame), shape = "square") +
    geom_hline(yintercept = -10:10, color = "lightgray", linetype = "dotted") +  # Grid lines
    geom_vline(xintercept = -10:10, color = "lightgray", linetype = "dotted") +  
    geom_hline(yintercept = 0, color = "black", linetype = "solid", size = 1) +  # X-axis
    geom_vline(xintercept = 0, color = "black", linetype = "solid", size = 1) +  # Y-axis
    scale_x_continuous(breaks = -10:10) + 
    scale_y_continuous(breaks = -10:10) +
    coord_cartesian(xlim = c(-10, 10), ylim = c(-10, 10)) +
    theme_minimal() +
    theme(panel.grid = element_blank()) +
    transition_states(frame, transition_length = 1, state_length = 1)

print(p_anim)

```

## Matrix properties and decomposition

### Prove that $AB \neq BA$

Matrices do not enjoy the commutative multiplicative property, unlike arithmetic algebra.

We can easily prove that $AB \neq BA$ empirically with a few lines of R code.

```{r}
A <- matrix(c(1,2,3,
              4,5,6,
              7,8,9), nrow=3, byrow=TRUE)

B <- matrix(c(1, 1, 1,
              2, 2, 2,
              3, 3, 3), nrow=3, byrow=TRUE)

AB <- A%*%B

BA <- B%*%B

print(all.equal(AB, BA))

```

### Prove that $A\^TA$ is always symmetric

A transpose of a matrix of rows $m$ and columns $n$ is simply re-ordering a matrix such that $A^T$ is composed of $n$ rows and $m$ columns. As such, the matrix maintains symmetry *with respect to the main diagonal*.

Again, we can show this using a concrete example, even with a non-square matrix.

```{r}

S <- matrix(c('a', 'b', 'c', 'x',
              'd', 'e', 'f', 'y',
              'g', 'h', 'i', 'z'), nrow=3, byrow=TRUE)

# transpose
print(t(S))

```

As we can see, the main diagonal of $a,e,i$ is retained, demonstrating symmetry throughout the transpose.

### Prove that the determinant of $A^TA$ is non-negative

The determinant is the scaling factor by which vector space is expanded after a matrix transformation. A negative determinant would indicate a reflection, meaning vector space is "stretched" towards either or both the negative x and y axis (given a standard two dimensional coordinate system).

Considering a two dimensional square matrix, $A=\begin{bmatrix}a&b\\c&d\end{bmatrix}$, we can calculate the determinant as $det|A| = A_{11}A_{22} - A_{12}A_{21} = ad - bc$.

Considering the transpose $A^T=\begin{bmatrix}a&c\\b&d\end{bmatrix}$, we can see that the determinant is non-negative given the commutative property of scalar multiplication; $det|A^T| = A^{T}_{11}A^{T}_{22} - A^{T}_{21}A^{T}_{12} = ad - cb$.

This is demonstrated in code below.

```{r}
A <- matrix(c(1, 2,
              3, 4), nrow=2, byrow=TRUE)

A_t <- t(A)

print(det(A_t %*% A))
```

### Image Singular Value Decomposition

We can use singular value decomposition, or SVD, to compress images. This works by analyzing the image as a matrix of pixels, each with a value of saturation. In the case of a color image, you would also have red, green, and blue matrices, but we will work with a simpler greyscale image in this example.

```{r}
library(magick)

# Read a grayscale image
image <- image_read('image.png')
grey_image <- image_convert(image, colorspace = 'gray')

image_matrix <- as.numeric(image_data(grey_image))

dim(image_matrix) <- c(image_info(grey_image)$width, image_info(grey_image)$height)

# Factorize the image matrix A using svd()

svd_matrix <- svd(image_matrix)

# Compress the image using top k singular values and vectors

k <- 10

compress_matrix <- svd_matrix$u[, 1:k] %*% diag(svd_matrix$d[1:k]) %*% t(svd_matrix$v[, 1:k])

# Visualize the result

compress_matrix_3d <- array(compress_matrix, dim = c(dim(compress_matrix)[1], dim(compress_matrix)[2], 3))

compress_image <- image_read(compress_matrix_3d)

print(image_append(c(grey_image, compress_image)))

```

## Matrix rank, properties, and eigenspace

### Determine the rank of the given matrix

> Find the rank of matrix A

*Note: we assume A is a coefficient matrix, not an augmented matrix in this example.*

In linear algebra, **rank** refers to the number of dimensions affected by a matrix transformation. If a matrix transformation retains all dimensions of a vector space, it is said to be **full rank**. A lesser rank occurs when a matrix transformation forces vector space into a smaller dimension; for example, a cube to a plane, a plane to a line, or a line to a point.

We calculate rank by first performing Gaussian elimination to get the matrix into reduced row echelon form. From there, the number of pivot columns is equivalent to the rank, as each pivot column represents linear independence for that given dimension.

```{r}
# Determine the rank of A
A <- matrix(c(2, 4, 1, 3,
              -2, -3, 4, 1,
              5, 6, 2, 8,
              -1, -2, 3, 7), nrow=4, byrow=TRUE)

A_rref <- echelon(A)

print(A_rref)

```

In this case, we can say that the rank of $A$ is four.

> Explain what the rank tells us about the linear independence of the rows and columns of A

A is full rank, as mentioned above, we have appropriate pivot values in each of our dimensions, which tells us the system is linearly independent and therefore full rank.

> Identify any linear dependencies among the rows or columns

If we saw any rows or columns that were strictly all zeroes, we could consider them to be free variables, and as such, linearly dependent.There are none in this case.

### Matrix rank boundaries

> Given an $mxn$ matrix where $m>n$, determine the maximum and minimum possible rank, assuming that the matrix is non-zero.

A matrix with more rows than columns indicate that we

We cannot have a higher rank than we have dimensions, so the maximum rank is $r_{max}\le n$

If the matrix is non-zero, the smallest it could possibly be is $1x1$. Therefore, rank must also be $r_{min}=1$.

> Prove that the rank of a matrix equals the dimension of its row space (or column space). Provide an example to illustrate the concept.

Given a linear system of equations $LS$ represented by matrix $A$ such that $LS(A)$;

1.  A homogeneous system $LS(A, 0)$ is always consistent
2.  The trivial ($0$) solution to $LS(A,0)$ is unique
3.  The reduced row echelon form $B$ of an augmented matrix $A$ is row equivalent
4.  $B$ has pivot columns $r$ which must be less than or equal to the number of columns $n$
5.  $r$ is equal to $n$ when the system has a unique solution
6.  $LS(A, 0)$ is consistent and has a unique solution, so for matrix $A$, $r$ must be equal to $n$
7.  Rank of A $r(A)$ is equal to $r$, so $r(A) = r_A = A_n$
8.  The transpose $A^T$ of matrix $A$ has rows $m$ and columns $n$ such that $A^{T}_{m} = A_n$
9.  The rank $r$ of a matrix is equal to the rank of the transpose, so $r_A = r_{A^T}$
10. $r_A = A_m$, or $r_A = A_n$

Example given $4x_1 + 2x_2  = 0$ and $3x_1 + x2 = 0$

```{r}
# Initialize matrix A
A <- matrix(c(4, 2,
              3, 1), nrow=2, byrow=TRUE)

# Compute RREF form B
B <- echelon(A)

# The number of pivot columns is X
print(B)

# Rank is equal to the number of rows
rank <- nrow(A)

# Transpose of A
A_t <- t(A)

# Assert r is equal to rows
print(identical(rank,nrow(A_t)))
```

### Rank and row reduction

> Determine the rank of matrix $B$. Perform a row reduction on matrix $B$ and describe how it helps in finding the rank. Discuss any special properties of matrix $B$ (e.g., is it a rank-deficient matrix?).

We can deduce the rank of $B$ by analyzing the number of dependent $D$ and free $F$ variables.

By reducing matrix $B$, we can examine its pivot columns. In this case, we can see that the first third columns have pivot values, while the second does not. This means the rank of $B$ is limited to two, the number of pivot columns. Given B is a 3x3 matrix, we can say that with a rank of two, it is rank-deficient (as opposed to full rank).

```{r}
# Initialize matrix
B <- matrix(c(2, 5, 7,
              4, 10, 14,
              1, 2.5, 2.5), nrow=3, byrow=TRUE)

# Perform row operations to achieve reduced row echelon form
B_rref <- echelon(B)

print(B_rref)
```

### Compute the eigenvalues and eigenvectors

> Find the eigenvalues and eigenvectors of the matrix $A$. Write out the characteristic polynomial and show your solution step by step. After finding the eigenvalues and eigenvectors, verify that the eigenvectors are linearly independent. If they are not, explain why.

$$
A =\begin{bmatrix}
3&1&2\\
0&5&4\\
0&0&2
\end{bmatrix}
$$

The roots of the characteristic polynomials of matrix $A$ will be our eigenvalues $\lambda$. We can then solve the system of equations for each value of $\lambda$ to calculate the eigenvectors, the span of which make up our eigenspace.

Detailed steps below. We will use the `pracma` and `matlib` packages to help with the computations.

Steps:
1. Determine characteristic polynomial as $p_A(\lambda)=det(A-\lambda I_3)$
2. Note the algebraic multiplicity of the characteristic polynomial
3. Solve the roots of the polynomial for all values of $\lambda$
4. Per value of $\lambda$, calculate $(A- \lambda I)\overrightarrow{v}=0$
5. Row reduce each $(A- \lambda I)$ to help find the free variables
6. Solve the row reduced matrix as a system of equations to determine an eigenvector by setting up $(A - \lambda I)\vec{v} = \vec{0}$
7. Repeat for each value of $\lambda$; the span of the resulting eigenvectors describe eigenspace
8. Test if eigenspace is linearly independent; if all our eigenvalues are distinct, we can say they are linearly independent. If we have repeated eigenvalues, we should test by confirming the determinant of the eigenvectors is non-zero
9. Check the dimensions of eigenspace to determine geometric multiplicity

```{r}
A <- matrix(c(3, 1, 2,
              0, 5, 4,
              0, 0, 2), nrow=3, byrow=TRUE)

# Step 1 characteristic polynomial
A_cp <- charpoly(A)

# Step 2 Algebraic multiplicity
alpha <- length(unique(A_cp))

# Step 3 Eigenvalues as roots of the CP
lambdas <- roots(A_cp)

# Step 4 Calculate eigenvectors per eigen value
# Note we are skipping to directly use the eigen() function for easier computation
# but we can easily confirm we have the same eigenvalues
eigen_A <- eigen(A)
print(eigen_A$values) # 5, 3, and 2, same as our polynomial roots

# Step 5, 6, and 7 are done for us
eigen_vec <- eigen_A$vectors

# Step 8 Test for linear independence by checking if any duplicates
print(duplicated(eigen_A$values)) # No duplicates so linearly independent

# Can validate by asserting the determinant of eigenspace is not zero
print(det(eigen_vec)=0)

# Step 9 The number of eigenvectors is our geometric multiplicity
gamma <- ncol(eigen_vec)
```

First we perform row operations to get the row reduced echelon form of $A$

$$
\begin{align}

\begin{bmatrix}
3&1&2\\
0&5&4\\
0&0&2
\end{bmatrix} \to -4R_1 + R_2

\begin{bmatrix}3&1&2\\0&1&4\\0&0&2\end{bmatrix} \\

-R_2 + R_1 \to

\begin{bmatrix}3&0&-2\\0&1&4\\0&0&2\end{bmatrix} \\

R_1 + R_3 \to

\begin{bmatrix}3&0&0\\0&1&4\\0&0&2\end{bmatrix} \\

\frac{1}{3}R_1 \to

\begin{bmatrix}1&0&0\\0&1&4\\0&0&2\end{bmatrix} \\

-2R_3 + R_2 \to

\begin{bmatrix}1&0&0\\0&1&0\\0&0&2\end{bmatrix} \\

\frac{1}{2}R_3 \to
\begin{bmatrix}1&0&0\\0&1&0\\0&0&1\end{bmatrix}
\end{align}
$$

In this case matrix $A$ can reduce to the identity matrix. We now determine the characteristic polynomial by calculating $p_A(x)=det(A-xI_3)$

$$
p_A(\lambda)=det(A-\lambda I_3) = det(\begin{bmatrix}1-\lambda&0&0\\0&1-\lambda&0\\0&0&1-\lambda\end{bmatrix})
$$

We have a diagonal matrix, so we can calculate the determinant as the product of its diagonal; $p_A(\lambda)=(1-x)(1-x)(1-x)$.

This is also our characteristic polynomial, and as expected, the eigenvalues are 1, which we would expect of any identity matrix. Since the eigenvalue of $1$ appears three times in our characteristic polynomial, we can also state the algebraic multiplicity is three.

Our eigenvectors can then be defined as $A\overrightarrow{x} = \lambda \overrightarrow{x}$. Since our matrix $A$ is an identity matrix, we can say $\overrightarrow{x} = \lambda \overrightarrow{x}$. Finally, since we have found $\lambda = 1$, we can say $\overrightarrow{x}=\overrightarrow{x}$, which is the same as saying that any vector of A is an eigenvector.

Since any vector of A is an eigenvector, we can say that the eigenvectors are linearly independent.This means they span all of their dimensional space, which in this case is $\mathbb{C}^3$. Due to this, we can also say that the geometric multiplicity is also three, as by definition it is equivalent to the dimensions of eigenspace.

### Diagonalization of matrix

> Determine if matrix $A$ can be diagonalized. If it can, find the diagonal matrix and the matrix of eigenvectors that diagonalizes $A$.

As a square matrix, $A$ is considered diagonalizable if it is similar to a diagonal matrix. 

> Discuss the geometric interpretation of the eigenvectors and eigenvalues in the context of transformations. For instance, how does matrix stretch, shrink, or rotate vectors in $\mathbb{R}^3$?

Eigenvectors represent the span that is unaffected by a geometric transformation. Consider a three dimensional rotation in any $x$, $y$, or $z$ direction; the axis of that rotation is itself the span of the eigenvectors. This is why we calculate eigenvalues using a zero determinant.

Some transformations result in all of vector space becoming the eigenspace, such as a streching or shrinking. In these cases, vectors are simply being multiplied by a scalar, which leaves their span unaffected. As such, the span of vector space is the same both before and after the scalar transformation.

In this way, eigenvalues can be thought of as "summaries" of a matrix transformation.

## Project: Eigenfaces from the "Labeled Faces in the Wild" dataset

>Preprocess the Images:
>Convert the images to grayscale and resize them to a smaller size (e.g., 64x64) to reduce computational complexity.
>Flatten each image into a vector.

>Apply PCA:         
>Compute the PCA on the flattened images.
>Determine the number of principal components required to account for 80% of the variability.

>Visualize Eigenfaces:
>Visualize the first few eigenfaces (principal components) and discuss their significance.
>Reconstruct some images using the computed eigenfaces and compare them with the original images.

I sampled four images from the dataset due to drive space, but the below code can be scaled to fit any number of images.

We are using the `magick` package again to help with the image processing. We read each image in the directory, preprocess it into greyscale and downscale to 64x64, then store the numerical representations in a list. Then it is simple to use the `prcomp` package to calculate the PCA summary for each image.

During my first attempts, I realized that some images had zero pixels (solid backgrounds being the suspected culprit). This meant we needed to perform some preprocessing to remove null elements so we could perform the PCA reliably.

This is necessary, as during the covariance matrix computation, you'll get zero variance which causes issues with the standardization for the rest of the PCA computation. I used Claude AI to help refactor my code so that we could dynamically and elegantly handle images that had this issue.

```{r}
library(magick)

# Read all iamges
image_vector_list <- list.files(path = "lfw/", 
                               pattern = "\\.(jpg)$",
                               full.names = TRUE) |>
  lapply(function(file) {
    # Read each image
    image_read(file) |>
    # Convert to greyscale
    image_convert(colorspace = 'gray') |>
    # Rescale image
    image_scale('64') |>
    # Get numeric image data
    image_data() |>
    as.numeric()
  })
```

```{r}
# Perform PCA on each vector
pca_list <- lapply(image_vector_list, function(image_vec) {
    # Reshape vector to matrix
    mat <- matrix(image_vec, nrow = 64, ncol = 64)
    
    # Check for columns with non-zero variance, otherwise we get error
    var_cols <- apply(mat, 2, var) > 0
    
    # If there are any columns with variance
    if(any(var_cols)) {
        # Only perform PCA on columns with variance
        mat_filtered <- mat[, var_cols, drop = FALSE]
        pca_result <- prcomp(mat_filtered, center = TRUE, scale. = TRUE)
        return(list(
            pca = pca_result,
            used_cols = which(var_cols)  # Keep track of which columns we used
        ))
    } else {
        # Return null if no variance in image as a check
        return(NULL)
    }
})

# Remove any null results (images where PCA couldn't be performed)
pca_list <- pca_list[!sapply(pca_list, is.null)]
```

With the PCA computed for each image, we can view the results and see for each how many principle components are need to explain more than 80% of the variability, by reviewing the `Cumulative Proportion` of each. Considering the first image as an example, we can see that we need five principal components in order to get to our benchmark.

```{r}
# The first image needs PC5
print(summary(pca_list[[1]]$pca))
```

Now to visualize how much information we get at each principal component, we will reconstruct the images at a series of $n$ components.

```{r}
# Function to reconstruct image from n principal components
reconstruct_image <- function(pca_result, n_components) {
    # Get the eigenvectors for first n components
    eigenvec <- pca_result$pca$rotation[, 1:n_components, drop = FALSE]
    
    # Get the scores for first n components
    scores <- pca_result$pca$x[, 1:n_components, drop = FALSE]
    
    # Reconstruct using matrix multiplication
    reconstructed <- scores %*% t(eigenvec)
    
    # Un-scale and un-center (reverse the standardization)
    # Check if scale is TRUE/FALSE rather than a vector
    if(!is.null(pca_result$pca$scale.) && pca_result$pca$scale.) {
        reconstructed <- scale(reconstructed, 
                             center = FALSE, 
                             scale = 1/pca_result$pca$sdev[1:n_components])
    }
    
    # Add back the center
    if(!is.null(pca_result$pca$center)) {
        reconstructed <- t(t(reconstructed) + pca_result$pca$center)
    }
    
    # Create full-size image matrix
    full_image <- matrix(0, nrow = 64, ncol = 64)
    full_image[, pca_result$used_cols] <- reconstructed
    
    return(full_image)
}

# Get first image's PCA result
first_pca <- pca_list[[1]]

# Original image for comparison
original_mat <- matrix(image_vector_list[[1]], nrow = 64, ncol = 64)

# Set up plotting parameters
par(mfrow = c(2,3))

# Original
image(original_mat, main = "Original")

# Reconstructions with different numbers of PCs
for(n_pc in c(1, 5, 10, 20, 50)) {
    reconstructed <- reconstruct_image(first_pca, n_pc)
    image(reconstructed, 
          main = paste("Using", n_pc, "PCs"))
}

# Calculate reconstruction error
for(n_pc in c(1, 5, 10, 20, 50)) {
    reconstructed <- reconstruct_image(first_pca, n_pc)
    mse <- mean((original_mat - reconstructed)^2)
    print(paste(n_pc, "PCs:", round(mse, 6)))
}
```

Comparing this to the original image, we can see how the PCA helps us identify the most important pixel values to retain the most visual information possible.

```{r}
print(image_vector_list[[1]])
```

